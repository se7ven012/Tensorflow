{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 16:07:37.919069 139705155954496 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-28 16:07:50.479869: step 0, duration = 0.210\n",
      "2019-08-28 16:07:52.580935: step 10, duration = 0.210\n",
      "2019-08-28 16:07:54.689086: step 20, duration = 0.211\n",
      "2019-08-28 16:07:56.798469: step 30, duration = 0.210\n",
      "2019-08-28 16:07:58.908889: step 40, duration = 0.212\n",
      "2019-08-28 16:08:01.018782: step 50, duration = 0.210\n",
      "2019-08-28 16:08:03.123886: step 60, duration = 0.211\n",
      "2019-08-28 16:08:05.236236: step 70, duration = 0.212\n",
      "2019-08-28 16:08:07.345108: step 80, duration = 0.211\n",
      "2019-08-28 16:08:09.458294: step 90, duration = 0.211\n",
      "2019-08-28 16:08:11.360134: Forward across 100 steps, 0.211 +/- 0.001 sec / batch\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "class Block(collections.namedtuple('Block', ['scope', 'unit_fn', 'args'])):\n",
    "  \"\"\"A named tuple describing a ResNet block.\n",
    "  Its parts are:\n",
    "    scope: The scope of the `Block`.\n",
    "    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n",
    "      returns another `Tensor` with the output of the ResNet unit.\n",
    "    args: A list of length equal to the number of units in the `Block`. The list\n",
    "      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n",
    "      block to serve as argument to unit_fn.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "def subsample(inputs, factor, scope=None):\n",
    "  \"\"\"Subsamples the input along the spatial dimensions.\n",
    "  Args:\n",
    "    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n",
    "    factor: The subsampling factor.\n",
    "    scope: Optional variable_scope.\n",
    "  Returns:\n",
    "    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n",
    "      input, either intact (if factor == 1) or subsampled (if factor > 1).\n",
    "  \"\"\"\n",
    "  if factor == 1:\n",
    "    return inputs\n",
    "  else:\n",
    "    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n",
    "\n",
    "\n",
    "def conv2d_same(inputs, num_outputs, kernel_size, stride, scope=None):\n",
    "  \"\"\"Strided 2-D convolution with 'SAME' padding.\n",
    "  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n",
    "  'VALID' padding.\n",
    "  Note that\n",
    "     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n",
    "  is equivalent to\n",
    "     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding='SAME')\n",
    "     net = subsample(net, factor=stride)\n",
    "  whereas\n",
    "     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding='SAME')\n",
    "  is different when the input's height or width is even, which is why we add the\n",
    "  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n",
    "  Args:\n",
    "    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n",
    "    num_outputs: An integer, the number of output filters.\n",
    "    kernel_size: An int with the kernel_size of the filters.\n",
    "    stride: An integer, the output stride.\n",
    "    rate: An integer, rate for atrous convolution.\n",
    "    scope: Scope.\n",
    "  Returns:\n",
    "    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n",
    "      the convolution output.\n",
    "  \"\"\"\n",
    "  if stride == 1:\n",
    "    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1,\n",
    "                       padding='SAME', scope=scope)\n",
    "  else:\n",
    "    #kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n",
    "    pad_total = kernel_size - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "    inputs = tf.pad(inputs,\n",
    "                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n",
    "    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n",
    "                       padding='VALID', scope=scope)\n",
    "\n",
    "\n",
    "@slim.add_arg_scope\n",
    "def stack_blocks_dense(net, blocks,\n",
    "                       outputs_collections=None):\n",
    "  \"\"\"Stacks ResNet `Blocks` and controls output feature density.\n",
    "  First, this function creates scopes for the ResNet in the form of\n",
    "  'block_name/unit_1', 'block_name/unit_2', etc.\n",
    "  Args:\n",
    "    net: A `Tensor` of size [batch, height, width, channels].\n",
    "    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n",
    "      element is a ResNet `Block` object describing the units in the `Block`.\n",
    "    outputs_collections: Collection to add the ResNet block outputs.\n",
    "  Returns:\n",
    "    net: Output tensor \n",
    "  \"\"\"\n",
    "  for block in blocks:\n",
    "    with tf.variable_scope(block.scope, 'block', [net]) as sc:\n",
    "      for i, unit in enumerate(block.args):\n",
    "\n",
    "        with tf.variable_scope('unit_%d' % (i + 1), values=[net]):\n",
    "          unit_depth, unit_depth_bottleneck, unit_stride = unit\n",
    "          net = block.unit_fn(net,\n",
    "                              depth=unit_depth,\n",
    "                              depth_bottleneck=unit_depth_bottleneck,\n",
    "                              stride=unit_stride)\n",
    "      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n",
    "      \n",
    "  return net\n",
    "\n",
    "\n",
    "def resnet_arg_scope(is_training=True,\n",
    "                     weight_decay=0.0001,\n",
    "                     batch_norm_decay=0.997,\n",
    "                     batch_norm_epsilon=1e-5,\n",
    "                     batch_norm_scale=True):\n",
    "  \"\"\"Defines the default ResNet arg scope.\n",
    "  TODO(gpapan): The batch-normalization related default values above are\n",
    "    appropriate for use in conjunction with the reference ResNet models\n",
    "    released at https://github.com/KaimingHe/deep-residual-networks. When\n",
    "    training ResNets from scratch, they might need to be tuned.\n",
    "  Args:\n",
    "    is_training: Whether or not we are training the parameters in the batch\n",
    "      normalization layers of the model.\n",
    "    weight_decay: The weight decay to use for regularizing the model.\n",
    "    batch_norm_decay: The moving average decay when estimating layer activation\n",
    "      statistics in batch normalization.\n",
    "    batch_norm_epsilon: Small constant to prevent division by zero when\n",
    "      normalizing activations by their variance in batch normalization.\n",
    "    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n",
    "      activations in the batch normalization layer.\n",
    "  Returns:\n",
    "    An `arg_scope` to use for the resnet models.\n",
    "  \"\"\"\n",
    "  batch_norm_params = {\n",
    "      'is_training': is_training,\n",
    "      'decay': batch_norm_decay,\n",
    "      'epsilon': batch_norm_epsilon,\n",
    "      'scale': batch_norm_scale,\n",
    "      'updates_collections': tf.GraphKeys.UPDATE_OPS,\n",
    "  }\n",
    "\n",
    "  with slim.arg_scope(\n",
    "      [slim.conv2d],\n",
    "      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "      weights_initializer=slim.variance_scaling_initializer(),\n",
    "      activation_fn=tf.nn.relu,\n",
    "      normalizer_fn=slim.batch_norm,\n",
    "      normalizer_params=batch_norm_params):\n",
    "    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
    "      # The following implies padding='SAME' for pool1, which makes feature\n",
    "      # alignment easier for dense prediction tasks. This is also used in\n",
    "      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n",
    "      # code of 'Deep Residual Learning for Image Recognition' uses\n",
    "      # padding='VALID' for pool1. You can switch to that choice by setting\n",
    "      # slim.arg_scope([slim.max_pool2d], padding='VALID').\n",
    "      with slim.arg_scope([slim.max_pool2d], padding='SAME') as arg_sc:\n",
    "        return arg_sc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@slim.add_arg_scope\n",
    "def bottleneck(inputs, depth, depth_bottleneck, stride,\n",
    "               outputs_collections=None, scope=None):\n",
    "  \"\"\"Bottleneck residual unit variant with BN before convolutions.\n",
    "  This is the full preactivation residual unit variant proposed in [2]. See\n",
    "  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n",
    "  variant which has an extra bottleneck layer.\n",
    "  When putting together two consecutive ResNet blocks that use this unit, one\n",
    "  should use stride = 2 in the last unit of the first block.\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, height, width, channels].\n",
    "    depth: The depth of the ResNet unit output.\n",
    "    depth_bottleneck: The depth of the bottleneck layers.\n",
    "    stride: The ResNet unit's stride. Determines the amount of downsampling of\n",
    "      the units output compared to its input.\n",
    "    rate: An integer, rate for atrous convolution.\n",
    "    outputs_collections: Collection to add the ResNet unit output.\n",
    "    scope: Optional variable_scope.\n",
    "  Returns:\n",
    "    The ResNet unit's output.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'bottleneck_v2', [inputs]) as sc:\n",
    "    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n",
    "    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope='preact')\n",
    "    if depth == depth_in:\n",
    "      shortcut = subsample(inputs, stride, 'shortcut')\n",
    "    else:\n",
    "      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n",
    "                             normalizer_fn=None, activation_fn=None,\n",
    "                             scope='shortcut')\n",
    "\n",
    "    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n",
    "                           scope='conv1')\n",
    "    residual = conv2d_same(residual, depth_bottleneck, 3, stride,\n",
    "                                        scope='conv2')\n",
    "    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n",
    "                           normalizer_fn=None, activation_fn=None,\n",
    "                           scope='conv3')\n",
    "\n",
    "    output = shortcut + residual\n",
    "\n",
    "    return slim.utils.collect_named_outputs(outputs_collections,\n",
    "                                            sc.name,\n",
    "                                            output)\n",
    "\n",
    "\n",
    "def resnet_v2(inputs,\n",
    "              blocks,\n",
    "              num_classes=None,\n",
    "              global_pool=True,\n",
    "              include_root_block=True,\n",
    "              reuse=None,\n",
    "              scope=None):\n",
    "  \"\"\"Generator for v2 (preactivation) ResNet models.\n",
    "  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n",
    "  methods for specific model instantiations, obtained by selecting different\n",
    "  block instantiations that produce ResNets of various depths.\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, height_in, width_in, channels].\n",
    "    blocks: A list of length equal to the number of ResNet blocks. Each element\n",
    "      is a resnet_utils.Block object describing the units in the block.\n",
    "    num_classes: Number of predicted classes for classification tasks. If None\n",
    "      we return the features before the logit layer.\n",
    "    include_root_block: If True, include the initial convolution followed by\n",
    "      max-pooling, if False excludes it. If excluded, `inputs` should be the\n",
    "      results of an activation-less convolution.\n",
    "    reuse: whether or not the network and its variables should be reused. To be\n",
    "      able to reuse 'scope' must be given.\n",
    "    scope: Optional variable_scope.\n",
    "  Returns:\n",
    "    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n",
    "      If global_pool is False, then height_out and width_out are reduced by a\n",
    "      factor of output_stride compared to the respective height_in and width_in,\n",
    "      else both height_out and width_out equal one. If num_classes is None, then\n",
    "      net is the output of the last ResNet block, potentially after global\n",
    "      average pooling. If num_classes is not None, net contains the pre-softmax\n",
    "      activations.\n",
    "    end_points: A dictionary from components of the network to the corresponding\n",
    "      activation.\n",
    "  Raises:\n",
    "    ValueError: If the target output_stride is not valid.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'resnet_v2', [inputs], reuse=reuse) as sc:\n",
    "    end_points_collection = sc.original_name_scope + '_end_points'\n",
    "    with slim.arg_scope([slim.conv2d, bottleneck,\n",
    "                         stack_blocks_dense],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = inputs\n",
    "      if include_root_block:\n",
    "        # We do not include batch normalization or activation functions in conv1\n",
    "        # because the first ResNet unit will perform these. Cf. Appendix of [2].\n",
    "        with slim.arg_scope([slim.conv2d],\n",
    "                            activation_fn=None, normalizer_fn=None):\n",
    "          net = conv2d_same(net, 64, 7, stride=2, scope='conv1')\n",
    "        net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1')\n",
    "      net = stack_blocks_dense(net, blocks)\n",
    "      # This is needed because the pre-activation variant does not have batch\n",
    "      # normalization or activation functions in the residual unit output. See\n",
    "      # Appendix of [2].\n",
    "      net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope='postnorm')\n",
    "      if global_pool:\n",
    "        # Global average pooling.\n",
    "        net = tf.reduce_mean(net, [1, 2], name='pool5', keep_dims=True)\n",
    "      if num_classes is not None:\n",
    "        net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n",
    "                          normalizer_fn=None, scope='logits')\n",
    "      # Convert end_points_collection into a dictionary of end_points.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if num_classes is not None:\n",
    "        end_points['predictions'] = slim.softmax(net, scope='predictions')\n",
    "      return net, end_points\n",
    "\n",
    "\n",
    "\n",
    "def resnet_v2_50(inputs,\n",
    "                 num_classes=None,\n",
    "                 global_pool=True,\n",
    "                 reuse=None,\n",
    "                 scope='resnet_v2_50'):\n",
    "  \"\"\"ResNet-50 model of [1]. See resnet_v2() for arg and return description.\"\"\"\n",
    "  blocks = [\n",
    "      Block('block1', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n",
    "      Block(\n",
    "          'block2', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n",
    "      Block(\n",
    "          'block3', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n",
    "      Block(\n",
    "          'block4', bottleneck, [(2048, 512, 1)] * 3)]\n",
    "  return resnet_v2(inputs, blocks, num_classes, global_pool,\n",
    "                   include_root_block=True, reuse=reuse, scope=scope)\n",
    "\n",
    "\n",
    "def resnet_v2_101(inputs,\n",
    "                  num_classes=None,\n",
    "                  global_pool=True,\n",
    "                  reuse=None,\n",
    "                  scope='resnet_v2_101'):\n",
    "  \"\"\"ResNet-101 model of [1]. See resnet_v2() for arg and return description.\"\"\"\n",
    "  blocks = [\n",
    "      Block(\n",
    "          'block1', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n",
    "      Block(\n",
    "          'block2', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n",
    "      Block(\n",
    "          'block3', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n",
    "      Block(\n",
    "          'block4', bottleneck, [(2048, 512, 1)] * 3)]\n",
    "  return resnet_v2(inputs, blocks, num_classes, global_pool,\n",
    "                   include_root_block=True, reuse=reuse, scope=scope)\n",
    "\n",
    "\n",
    "def resnet_v2_152(inputs,\n",
    "                  num_classes=None,\n",
    "                  global_pool=True,\n",
    "                  reuse=None,\n",
    "                  scope='resnet_v2_152'):\n",
    "  \"\"\"ResNet-152 model of [1]. See resnet_v2() for arg and return description.\"\"\"\n",
    "  blocks = [\n",
    "      Block(\n",
    "          'block1', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n",
    "      Block(\n",
    "          'block2', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n",
    "      Block(\n",
    "          'block3', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n",
    "      Block(\n",
    "          'block4', bottleneck, [(2048, 512, 1)] * 3)]\n",
    "  return resnet_v2(inputs, blocks, num_classes, global_pool,\n",
    "                   include_root_block=True, reuse=reuse, scope=scope)\n",
    "\n",
    "\n",
    "def resnet_v2_200(inputs,\n",
    "                  num_classes=None,\n",
    "                  global_pool=True,\n",
    "                  reuse=None,\n",
    "                  scope='resnet_v2_200'):\n",
    "  \"\"\"ResNet-200 model of [2]. See resnet_v2() for arg and return description.\"\"\"\n",
    "  blocks = [\n",
    "      Block(\n",
    "          'block1', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n",
    "      Block(\n",
    "          'block2', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n",
    "      Block(\n",
    "          'block3', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n",
    "      Block(\n",
    "          'block4', bottleneck, [(2048, 512, 1)] * 3)]\n",
    "  return resnet_v2(inputs, blocks, num_classes, global_pool,\n",
    "                   include_root_block=True, reuse=reuse, scope=scope)\n",
    "\n",
    "  \n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "def time_tensorflow_run(session, target, info_string):\n",
    "    num_steps_burn_in = 10\n",
    "    total_duration = 0.0\n",
    "    total_duration_squared = 0.0\n",
    "    for i in range(num_batches + num_steps_burn_in):\n",
    "        start_time = time.time()\n",
    "        _ = session.run(target)\n",
    "        duration = time.time() - start_time\n",
    "        if i >= num_steps_burn_in:\n",
    "            if not i % 10:\n",
    "                print ('%s: step %d, duration = %.3f' %\n",
    "                       (datetime.now(), i - num_steps_burn_in, duration))\n",
    "            total_duration += duration\n",
    "            total_duration_squared += duration * duration\n",
    "    mn = total_duration / num_batches\n",
    "    vr = total_duration_squared / num_batches - mn * mn\n",
    "    sd = math.sqrt(vr)\n",
    "    print ('%s: %s across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "           (datetime.now(), info_string, num_batches, mn, sd))\n",
    "    \n",
    "batch_size = 32\n",
    "height, width = 224, 224\n",
    "inputs = tf.random_uniform((batch_size, height, width, 3))\n",
    "with slim.arg_scope(resnet_arg_scope(is_training=False)):\n",
    "   net, end_points = resnet_v2_152(inputs, 1000)\n",
    "  \n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)  \n",
    "num_batches=100\n",
    "time_tensorflow_run(sess, net, \"Forward\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
