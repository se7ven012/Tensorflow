{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow will takes over all the memory capacity during processing but computing on only hone GPU.\n",
    "#GPU resources limitation\n",
    "gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "#0.333 is the percentage of Memory Capacity which will be used\n",
    "config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-76f47d58ca1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#compute A^n and store result in c2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "#GPU id（0到n-1,其中n是机器上GPU卡的数量）\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 15:47:23.168834 140510538295104 deprecation.py:323] From <ipython-input-1-ce6c8aa764cb>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0826 15:47:23.169934 140510538295104 deprecation.py:323] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0826 15:47:23.170762 140510538295104 deprecation.py:323] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0826 15:47:23.363476 140510538295104 deprecation.py:323] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0826 15:47:23.365758 140510538295104 deprecation.py:323] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 15:47:23.404805 140510538295104 deprecation.py:323] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 15:47:23.854383 140510538295104 deprecation.py:323] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0826 15:47:24.375272 140510538295104 deprecation.py:323] From <ipython-input-1-ce6c8aa764cb>:41: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:1:2.1768055 0.10058594\n",
      "Step:10:1.7420014 0.111328125\n",
      "Step:20:1.3150971 0.11230469\n",
      "Step:30:0.9656034 0.11230469\n",
      "Step:40:0.71995413 0.12792969\n",
      "Step:50:0.54954606 0.29492188\n",
      "Step:60:0.429086 0.8701172\n",
      "Step:70:0.33470196 0.96972656\n",
      "Step:80:0.29061007 0.97558594\n",
      "Step:90:0.25628546 0.9765625\n",
      "Step:100:0.21566352 0.98535156\n",
      "Step:110:0.19937478 0.9785156\n",
      "Step:120:0.18681864 0.97753906\n",
      "Step:130:0.1634177 0.984375\n",
      "Step:140:0.16077583 0.984375\n",
      "Step:150:0.14618132 0.9863281\n",
      "Step:160:0.14127463 0.9814453\n",
      "Step:170:0.13692878 0.98535156\n",
      "Step:180:0.115137175 0.98828125\n",
      "Step:190:0.107293114 0.99121094\n",
      "Step:200:0.10355579 0.98535156\n",
      "Done\n",
      "Testing Accuracy: 0.9869978\n"
     ]
    }
   ],
   "source": [
    "#single GPU demo\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    " \n",
    "mnist=input_data.read_data_sets(\"/tmp/mnist/\",one_hot=True)\n",
    " \n",
    "num_gpus=2\n",
    "num_steps=200\n",
    "learning_rate=0.001\n",
    "batch_size=1024\n",
    "display_step=10\n",
    " \n",
    "num_input=784\n",
    "num_classes=10\n",
    "def conv_net(x,is_training):\n",
    "    # \"updates_collections\": None is very import ,without will only get 0.10\n",
    "    batch_norm_params = {\"is_training\": is_training, \"decay\": 0.9, \"updates_collections\": None}\n",
    "    #,'variables_collections': [ tf.GraphKeys.TRAINABLE_VARIABLES ]\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        weights_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
    "                        weights_regularizer=slim.l2_regularizer(0.0005),\n",
    "                        normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params):\n",
    "        with tf.variable_scope(\"ConvNet\",reuse=tf.AUTO_REUSE):\n",
    "            x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "            net = slim.conv2d(x, 6, [5,5], scope=\"conv_1\")\n",
    "            net = slim.max_pool2d(net, [2, 2],scope=\"pool_1\")\n",
    "            net = slim.conv2d(net, 12, [5,5], scope=\"conv_2\")\n",
    "            net = slim.max_pool2d(net, [2, 2], scope=\"pool_2\")\n",
    "            net = slim.flatten(net, scope=\"flatten\")\n",
    "            net = slim.fully_connected(net, 100, scope=\"fc\")\n",
    "            net = slim.dropout(net,is_training=is_training)\n",
    "            net = slim.fully_connected(net, num_classes, scope=\"prob\", activation_fn=None,normalizer_fn=None)\n",
    "            return net\n",
    "def train_single():\n",
    "    X = tf.placeholder(tf.float32, [None, num_input])\n",
    "    Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "    logits=conv_net(X,True)\n",
    "    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=logits))\n",
    "    opt=tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op=opt.minimize(loss)\n",
    "    logits_test=conv_net(X,False)\n",
    "    correct_prediction = tf.equal(tf.argmax(logits_test, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for step in range(1,num_steps+1):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_op,feed_dict={X:batch_x,Y:batch_y})\n",
    "            if step%display_step==0 or step==1:\n",
    "                loss_value,acc=sess.run([loss,accuracy],feed_dict={X:batch_x,Y:batch_y})\n",
    "                print(\"Step:\" + str(step) + \":\" + str(loss_value) + \" \" + str(acc))\n",
    "        print(\"Done\")\n",
    "        print(\"Testing Accuracy:\",np.mean([sess.run(accuracy, feed_dict={X: mnist.test.images[i:i + batch_size],\n",
    "              Y: mnist.test.labels[i:i + batch_size]}) for i in\n",
    "              range(0, len(mnist.test.images), batch_size)]))\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    train_single()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多GPU并行可分为模型并行和数据并行两大类,而其中数据并行又可分为同步方式和异步方式两种\n",
    "#由于我们一般都会配置同样的显卡，因此这儿也选择了同步方式，也就是把数据分给不同的卡\n",
    "#等所有的GPU都计算完梯度后进行平均，最后再更新梯度\n",
    "\n",
    "#这里是数据部分 现在有多张卡 每张卡要分得不同的数据\n",
    "#所以 batch_x,batch_y=mnist.train.next_batch(batch_size*num_gpus)\n",
    "#这里保证每张卡都能获得batch_size大小的数据\n",
    "#接下来切分数据 i为GPU索引 连续的batch_size大小数据分给同块卡\n",
    "_x=X[i*batch_size:(i+1)*batch_size]\n",
    "_y=Y[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "#因为在多个GPU上共享同一张图 为防止名字混乱 用name_scope进行区分\n",
    "        for i in range(2):\n",
    "            with tf.device(\"/gpu:%d\"%i):\n",
    "                with tf.name_scope(\"tower_%d\"%i):\n",
    "                    _x=X[i*batch_size:(i+1)*batch_size]\n",
    "                    _y=Y[i*batch_size:(i+1)*batch_size]\n",
    "                    logits=conv_net(_x,dropout,reuse_vars,True)\n",
    "\n",
    "#这里需要一个列表储存所有GPU的梯度，另外需要定义复用变量\n",
    "tower_grads=[]\n",
    "reuse_vars=False\n",
    "\n",
    "#接下来开始计算各GPU的梯度\n",
    "                    opt = tf.train.AdamOptimizer(learning_rate)\n",
    "                    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_y,logits=logits))\n",
    "                    grads=opt.compute_gradients(loss)\n",
    "                    reuse_vars=True\n",
    "                    tower_grads.append(grads)\n",
    "\n",
    "#接下来求平均梯度\n",
    "#Grads has been stored in tower_grads as [grads on gpu0, grads on gpu1]\n",
    "#Notice that  zip(*) is basically transforming the above array\n",
    "#to ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "#So that we can get for example the values of var0 on different GPUs\n",
    "def average_gradients(tower_grads):\n",
    "    average_grads=[]\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        grads=[]\n",
    "        for g,_ in grad_and_vars:\n",
    "            expend_g=tf.expand_dims(g,0)\n",
    "            grads.append(expend_g)\n",
    "        grad=tf.concat(grads,0)\n",
    "        grad=tf.reduce_mean(grad,0)\n",
    "        v=grad_and_vars[0][1]\n",
    "        grad_and_var=(grad,v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads\n",
    "\n",
    "#接下来更新梯度\n",
    "grads=average_gradients(tower_grads)\n",
    "train_op=opt.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Available GPU Number :2\n",
      "Step:1:2.1677146 0.087, 12108 Examples/sec\n",
      "Step:10:1.6895834 0.102, 142653 Examples/sec\n",
      "Step:20:1.275157 0.121, 127871 Examples/sec\n",
      "Step:30:0.9419908 0.848, 164954 Examples/sec\n",
      "Step:40:0.66217387 0.959, 162070 Examples/sec\n",
      "Step:50:0.50352776 0.974, 155658 Examples/sec\n",
      "Step:60:0.38034534 0.97, 139347 Examples/sec\n",
      "Step:70:0.3190288 0.983, 162939 Examples/sec\n",
      "Step:80:0.26566234 0.984, 138118 Examples/sec\n",
      "Step:90:0.21712294 0.981, 169875 Examples/sec\n",
      "Step:100:0.20165718 0.991, 169645 Examples/sec\n",
      "Step:110:0.16454723 0.989, 156995 Examples/sec\n",
      "Step:120:0.16735114 0.987, 160513 Examples/sec\n",
      "Step:130:0.1390191 0.987, 162098 Examples/sec\n",
      "Step:140:0.12545046 0.986, 165828 Examples/sec\n",
      "Step:150:0.14234646 0.988, 169518 Examples/sec\n",
      "Step:160:0.1269624 0.992, 155650 Examples/sec\n",
      "Step:170:0.1038568 0.992, 111958 Examples/sec\n",
      "Step:180:0.11035139 0.991, 134908 Examples/sec\n",
      "Step:190:0.09301638 0.986, 126940 Examples/sec\n",
      "Step:200:0.08959153 0.99, 146720 Examples/sec\n",
      "Step:210:0.08406594 0.991, 166038 Examples/sec\n",
      "Step:220:0.08239991 0.987, 151331 Examples/sec\n",
      "Step:230:0.08264324 0.988, 167043 Examples/sec\n",
      "Step:240:0.06974208 0.988, 162812 Examples/sec\n",
      "Step:250:0.069852315 0.995, 173540 Examples/sec\n",
      "Step:260:0.06898469 0.993, 172045 Examples/sec\n",
      "Step:270:0.061722945 0.99, 144800 Examples/sec\n",
      "Step:280:0.06643125 0.995, 129381 Examples/sec\n",
      "Step:290:0.05338119 0.99, 157500 Examples/sec\n",
      "Step:300:0.06485131 0.991, 160415 Examples/sec\n",
      "Step:310:0.060226098 0.989, 171799 Examples/sec\n",
      "Step:320:0.06391607 0.995, 164932 Examples/sec\n",
      "Step:330:0.06263532 0.991, 152797 Examples/sec\n",
      "Step:340:0.05996813 0.994, 128325 Examples/sec\n",
      "Step:350:0.05478157 0.997, 150746 Examples/sec\n",
      "Step:360:0.059860487 0.995, 171021 Examples/sec\n",
      "Step:370:0.05354466 0.993, 165045 Examples/sec\n",
      "Step:380:0.054554883 0.993, 141136 Examples/sec\n",
      "Step:390:0.046632428 0.994, 169388 Examples/sec\n",
      "Step:400:0.043198656 0.995, 169803 Examples/sec\n",
      "Step:410:0.0442918 0.996, 161381 Examples/sec\n",
      "Step:420:0.045151684 0.996, 168111 Examples/sec\n",
      "Step:430:0.042035867 0.999, 124852 Examples/sec\n",
      "Step:440:0.039201606 0.997, 142119 Examples/sec\n",
      "Step:450:0.041499134 0.997, 98040 Examples/sec\n",
      "Step:460:0.048920136 0.997, 154994 Examples/sec\n",
      "Step:470:0.0458356 0.996, 160219 Examples/sec\n",
      "Step:480:0.035960827 0.997, 173907 Examples/sec\n",
      "Step:490:0.037639372 0.993, 152820 Examples/sec\n",
      "Step:500:0.04549688 0.999, 165039 Examples/sec\n",
      "Step:510:0.039364617 0.997, 169459 Examples/sec\n",
      "Step:520:0.046283554 0.994, 159999 Examples/sec\n",
      "Step:530:0.04415293 0.999, 114309 Examples/sec\n",
      "Step:540:0.0432575 0.998, 129539 Examples/sec\n",
      "Step:550:0.038304847 0.992, 161989 Examples/sec\n",
      "Step:560:0.036736544 0.997, 172183 Examples/sec\n",
      "Step:570:0.040931158 0.998, 163776 Examples/sec\n",
      "Step:580:0.03757626 0.998, 148950 Examples/sec\n",
      "Step:590:0.03816831 0.998, 153432 Examples/sec\n",
      "Step:600:0.03162491 0.998, 160710 Examples/sec\n",
      "Step:610:0.030110946 0.998, 154680 Examples/sec\n",
      "Step:620:0.025553972 0.997, 165740 Examples/sec\n",
      "Step:630:0.03972831 0.999, 150996 Examples/sec\n",
      "Step:640:0.035593733 0.999, 147955 Examples/sec\n",
      "Step:650:0.039141465 1.0, 146789 Examples/sec\n",
      "Step:660:0.029509282 0.993, 165592 Examples/sec\n",
      "Step:670:0.033383455 0.998, 135639 Examples/sec\n",
      "Step:680:0.029828237 0.998, 169559 Examples/sec\n",
      "Step:690:0.031142134 0.999, 161599 Examples/sec\n",
      "Step:700:0.028822435 0.998, 169415 Examples/sec\n",
      "Step:710:0.02253328 0.999, 160880 Examples/sec\n",
      "Step:720:0.0280876 0.996, 166051 Examples/sec\n",
      "Step:730:0.02586429 0.996, 145166 Examples/sec\n",
      "Step:740:0.027750669 0.999, 146114 Examples/sec\n",
      "Step:750:0.030976584 1.0, 121178 Examples/sec\n",
      "Step:760:0.029617153 0.994, 164582 Examples/sec\n",
      "Step:770:0.048554786 0.999, 155733 Examples/sec\n",
      "Step:780:0.033968724 1.0, 116730 Examples/sec\n",
      "Step:790:0.023446437 0.997, 156163 Examples/sec\n",
      "Step:800:0.024693038 0.998, 142114 Examples/sec\n",
      "Step:810:0.025528526 1.0, 127001 Examples/sec\n",
      "Step:820:0.024621112 0.999, 156907 Examples/sec\n",
      "Step:830:0.027944058 1.0, 159634 Examples/sec\n",
      "Step:840:0.02828412 0.999, 168088 Examples/sec\n",
      "Step:850:0.022748677 0.999, 146405 Examples/sec\n",
      "Step:860:0.02639215 0.998, 154191 Examples/sec\n",
      "Step:870:0.02174285 0.997, 162541 Examples/sec\n",
      "Step:880:0.02856868 0.999, 159993 Examples/sec\n",
      "Step:890:0.020289553 0.999, 143319 Examples/sec\n",
      "Step:900:0.024776993 0.998, 153567 Examples/sec\n",
      "Step:910:0.022403523 0.997, 150514 Examples/sec\n",
      "Step:920:0.019206168 0.999, 166808 Examples/sec\n",
      "Step:930:0.03666904 0.999, 154324 Examples/sec\n",
      "Step:940:0.02154323 1.0, 165661 Examples/sec\n",
      "Step:950:0.024202822 0.998, 173150 Examples/sec\n",
      "Step:960:0.021371912 0.999, 161025 Examples/sec\n",
      "Step:970:0.022786759 0.996, 142392 Examples/sec\n",
      "Step:980:0.0207435 0.998, 158592 Examples/sec\n",
      "Step:990:0.018881995 0.997, 144981 Examples/sec\n",
      "Step:1000:0.027490707 0.999, 138291 Examples/sec\n",
      "Done\n",
      "Testing Accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "#multiple GPU demo\n",
    "import time\n",
    "import numpy as np\n",
    " \n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    " \n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)\n",
    "\n",
    "\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib as _device_lib\n",
    "    local_device_protos = _device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    " \n",
    "num_gpus = len(get_available_gpus())\n",
    "print(\"Available GPU Number :\"+str(num_gpus))\n",
    " \n",
    "num_steps = 1000\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "display_step = 10\n",
    " \n",
    "num_input = 784\n",
    "num_classes = 10\n",
    " \n",
    "def conv_net_with_layers(x,is_training,dropout = 0.75):\n",
    "    with tf.variable_scope(\"ConvNet\", reuse=tf.AUTO_REUSE):\n",
    "        x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        x = tf.layers.conv2d(x, 12, 5, activation=tf.nn.relu)\n",
    "        x = tf.layers.max_pooling2d(x, 2, 2)\n",
    "        x = tf.layers.conv2d(x, 24, 3, activation=tf.nn.relu)\n",
    "        x = tf.layers.max_pooling2d(x, 2, 2)\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, 100)\n",
    "        x = tf.layers.dropout(x, rate=dropout, training=is_training)\n",
    "        out = tf.layers.dense(x, 10)\n",
    "        out = tf.nn.softmax(out) if not is_training else out\n",
    "    return out\n",
    " \n",
    "def conv_net(x,is_training):\n",
    "    # \"updates_collections\": None is very import ,without will only get 0.10\n",
    "    batch_norm_params = {\"is_training\": is_training, \"decay\": 0.9, \"updates_collections\": None}\n",
    "    #,'variables_collections': [ tf.GraphKeys.TRAINABLE_VARIABLES ]\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        weights_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
    "                        weights_regularizer=slim.l2_regularizer(0.0005),\n",
    "                        normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params):\n",
    "        with tf.variable_scope(\"ConvNet\",reuse=tf.AUTO_REUSE):\n",
    "            x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "            net = slim.conv2d(x, 6, [5,5], scope=\"conv_1\")\n",
    "            net = slim.max_pool2d(net, [2, 2],scope=\"pool_1\")\n",
    "            net = slim.conv2d(net, 12, [5,5], scope=\"conv_2\")\n",
    "            net = slim.max_pool2d(net, [2, 2], scope=\"pool_2\")\n",
    "            net = slim.flatten(net, scope=\"flatten\")\n",
    "            net = slim.fully_connected(net, 100, scope=\"fc\")\n",
    "            net = slim.dropout(net,is_training=is_training)\n",
    "            net = slim.fully_connected(net, num_classes, scope=\"prob\", activation_fn=None,normalizer_fn=None)\n",
    "            return net\n",
    " \n",
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            expend_g = tf.expand_dims(g, 0)\n",
    "            grads.append(expend_g)\n",
    "        grad = tf.concat(grads, 0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads\n",
    " \n",
    "PS_OPS = ['Variable', 'VariableV2', 'AutoReloadVariable']\n",
    "def assign_to_device(device, ps_device='/cpu:0'):\n",
    "    def _assign(op):\n",
    "        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n",
    "        if node_def.op in PS_OPS:\n",
    "            return \"/\" + ps_device\n",
    "        else:\n",
    "            return device\n",
    " \n",
    "    return _assign\n",
    " \n",
    "def trainDouble():\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        global_step=tf.train.get_or_create_global_step()\n",
    "        tower_grads = []\n",
    "        X = tf.placeholder(tf.float32, [None, num_input])\n",
    "        Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        with tf.variable_scope(tf.get_variable_scope()):\n",
    "            for i in range(num_gpus):\n",
    "                with tf.device(assign_to_device('/gpu:{}'.format(i), ps_device='/cpu:0')):\n",
    "                    _x = X[i * batch_size:(i + 1) * batch_size]\n",
    "                    _y = Y[i * batch_size:(i + 1) * batch_size]\n",
    "                    logits = conv_net(_x, True)\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_y, logits=logits))\n",
    "                    grads = opt.compute_gradients(loss)\n",
    "                    tower_grads.append(grads)\n",
    "                    if i == 0:\n",
    "                        logits_test = conv_net(_x, False)\n",
    "                        correct_prediction = tf.equal(tf.argmax(logits_test, 1), tf.argmax(_y, 1))\n",
    "                        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        grads = average_gradients(tower_grads)\n",
    "        train_op = opt.apply_gradients(grads)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for step in range(1, num_steps + 1):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size * num_gpus)\n",
    "                ts = time.time()\n",
    "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "                te = time.time() - ts\n",
    "                if step % 10 == 0 or step == 1:\n",
    "                    loss_value, acc = sess.run([loss, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "                    print(\"Step:\" + str(step) + \":\" + str(loss_value) + \" \" + str(acc)+\", %i Examples/sec\" % int(len(batch_x)/te))\n",
    "            print(\"Done\")\n",
    "            print(\"Testing Accuracy:\",\n",
    "                  np.mean([sess.run(accuracy, feed_dict={X: mnist.test.images[i:i + batch_size],\n",
    "                                                         Y: mnist.test.labels[i:i + batch_size]}) for i in\n",
    "                           range(0, len(mnist.test.images), batch_size)]))\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    #train_single()\n",
    "    trainDouble()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
