{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = \n",
      "[[ 0  1]\n",
      " [ 1 -1]]\n",
      "z1 = \n",
      "[[-1  0]\n",
      " [ 0 -1]]\n",
      "z2 = \n",
      "[[1 0]\n",
      " [0 1]]\n",
      "\n",
      "\n",
      "[4]\n",
      "[2 1]\n"
     ]
    }
   ],
   "source": [
    "#testing subtract\n",
    "\n",
    "#two matrix subtract\n",
    "x=tf.constant([[1,2],[2,1]])  \n",
    "y=tf.constant([[1,1],[1,2]])\n",
    "z=tf.subtract(x,y)\n",
    "#matrix subtract a constant\n",
    "x1=tf.constant([[1,2],[2,1]])  \n",
    "y1=tf.constant(2)\n",
    "z1=tf.subtract(x1,y1)\n",
    "#a constant subtract a matrix\n",
    "x2=tf.constant(2)\n",
    "y2=tf.constant([[1,2],[2,1]])\n",
    "z2=tf.subtract(x2,y2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    z_v,z1_v,z2_v=sess.run([z,z1,z2])\n",
    "    print('z = \\n%s'%(z_v))\n",
    "    print('z1 = \\n%s'%(z1_v))\n",
    "    print('z2 = \\n%s'%(z2_v))\n",
    "    \n",
    "#testing argmax\n",
    "\n",
    "A = [[1,3,4,5,6]]\n",
    "B = [[1,3,4], [2,4,1]]\n",
    " \n",
    "with tf.Session() as sess:\n",
    "    print('\\n')\n",
    "    print(sess.run(tf.argmax(A, 1)))\n",
    "    print(sess.run(tf.argmax(B, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 16:08:13.155298 140125394077504 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0827 16:08:13.350167 140125394077504 deprecation.py:323] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 25 , average reward of last 25 episode 2.16\n",
      "episode 50 , average reward of last 25 episode 3.0\n",
      "episode 75 , average reward of last 25 episode 1.8\n",
      "episode 100 , average reward of last 25 episode 1.36\n",
      "episode 125 , average reward of last 25 episode 1.96\n",
      "episode 150 , average reward of last 25 episode 2.0\n",
      "episode 175 , average reward of last 25 episode 1.8\n",
      "episode 200 , average reward of last 25 episode 1.96\n",
      "episode 225 , average reward of last 25 episode 1.8\n",
      "episode 250 , average reward of last 25 episode 3.32\n",
      "episode 275 , average reward of last 25 episode 5.12\n",
      "episode 300 , average reward of last 25 episode 5.68\n",
      "episode 325 , average reward of last 25 episode 8.56\n",
      "episode 350 , average reward of last 25 episode 9.64\n",
      "episode 375 , average reward of last 25 episode 11.6\n",
      "episode 400 , average reward of last 25 episode 15.04\n",
      "episode 425 , average reward of last 25 episode 15.44\n",
      "episode 450 , average reward of last 25 episode 18.52\n",
      "episode 475 , average reward of last 25 episode 21.4\n",
      "episode 500 , average reward of last 25 episode 21.92\n",
      "episode 525 , average reward of last 25 episode 21.84\n",
      "episode 550 , average reward of last 25 episode 22.08\n",
      "episode 575 , average reward of last 25 episode 22.12\n",
      "episode 600 , average reward of last 25 episode 22.52\n",
      "episode 625 , average reward of last 25 episode 22.48\n",
      "episode 650 , average reward of last 25 episode 22.88\n",
      "episode 675 , average reward of last 25 episode 22.68\n",
      "episode 700 , average reward of last 25 episode 22.56\n",
      "episode 725 , average reward of last 25 episode 21.6\n",
      "episode 750 , average reward of last 25 episode 22.88\n",
      "episode 775 , average reward of last 25 episode 23.72\n",
      "episode 800 , average reward of last 25 episode 22.2\n",
      "episode 825 , average reward of last 25 episode 22.72\n",
      "episode 850 , average reward of last 25 episode 21.8\n",
      "episode 875 , average reward of last 25 episode 22.68\n",
      "episode 900 , average reward of last 25 episode 22.0\n",
      "episode 925 , average reward of last 25 episode 22.0\n",
      "episode 950 , average reward of last 25 episode 22.68\n",
      "episode 975 , average reward of last 25 episode 22.12\n",
      "episode 1000 , average reward of last 25 episode 22.64\n",
      "Saved Model\n",
      "episode 1025 , average reward of last 25 episode 21.88\n",
      "episode 1050 , average reward of last 25 episode 22.32\n",
      "episode 1075 , average reward of last 25 episode 23.92\n",
      "episode 1100 , average reward of last 25 episode 21.88\n",
      "episode 1125 , average reward of last 25 episode 22.24\n",
      "episode 1150 , average reward of last 25 episode 22.6\n",
      "episode 1175 , average reward of last 25 episode 22.48\n",
      "episode 1200 , average reward of last 25 episode 22.28\n",
      "episode 1225 , average reward of last 25 episode 23.0\n",
      "episode 1250 , average reward of last 25 episode 22.72\n",
      "episode 1275 , average reward of last 25 episode 23.88\n",
      "episode 1300 , average reward of last 25 episode 22.6\n",
      "episode 1325 , average reward of last 25 episode 23.08\n",
      "episode 1350 , average reward of last 25 episode 23.4\n",
      "episode 1375 , average reward of last 25 episode 22.04\n",
      "episode 1400 , average reward of last 25 episode 23.16\n",
      "episode 1425 , average reward of last 25 episode 22.36\n",
      "episode 1450 , average reward of last 25 episode 23.68\n",
      "episode 1475 , average reward of last 25 episode 21.0\n",
      "episode 1500 , average reward of last 25 episode 22.28\n",
      "episode 1525 , average reward of last 25 episode 23.04\n",
      "episode 1550 , average reward of last 25 episode 22.36\n",
      "episode 1575 , average reward of last 25 episode 22.68\n",
      "episode 1600 , average reward of last 25 episode 22.28\n",
      "episode 1625 , average reward of last 25 episode 21.8\n",
      "episode 1650 , average reward of last 25 episode 22.4\n",
      "episode 1675 , average reward of last 25 episode 23.4\n",
      "episode 1700 , average reward of last 25 episode 22.52\n",
      "episode 1725 , average reward of last 25 episode 21.8\n",
      "episode 1750 , average reward of last 25 episode 21.96\n",
      "episode 1775 , average reward of last 25 episode 22.8\n",
      "episode 1800 , average reward of last 25 episode 21.6\n",
      "episode 1825 , average reward of last 25 episode 22.36\n",
      "episode 1850 , average reward of last 25 episode 22.32\n",
      "episode 1875 , average reward of last 25 episode 23.12\n",
      "episode 1900 , average reward of last 25 episode 22.76\n",
      "episode 1925 , average reward of last 25 episode 23.36\n",
      "episode 1950 , average reward of last 25 episode 22.12\n",
      "episode 1975 , average reward of last 25 episode 22.16\n",
      "episode 2000 , average reward of last 25 episode 21.68\n",
      "Saved Model\n",
      "episode 2025 , average reward of last 25 episode 21.36\n",
      "episode 2050 , average reward of last 25 episode 23.28\n",
      "episode 2075 , average reward of last 25 episode 22.76\n",
      "episode 2100 , average reward of last 25 episode 22.8\n",
      "episode 2125 , average reward of last 25 episode 22.88\n",
      "episode 2150 , average reward of last 25 episode 21.48\n",
      "episode 2175 , average reward of last 25 episode 22.48\n",
      "episode 2200 , average reward of last 25 episode 22.12\n",
      "episode 2225 , average reward of last 25 episode 21.88\n",
      "episode 2250 , average reward of last 25 episode 23.0\n",
      "episode 2275 , average reward of last 25 episode 22.56\n",
      "episode 2300 , average reward of last 25 episode 21.24\n",
      "episode 2325 , average reward of last 25 episode 22.2\n",
      "episode 2350 , average reward of last 25 episode 21.72\n",
      "episode 2375 , average reward of last 25 episode 21.36\n",
      "episode 2400 , average reward of last 25 episode 22.2\n",
      "episode 2425 , average reward of last 25 episode 22.56\n",
      "episode 2450 , average reward of last 25 episode 22.0\n",
      "episode 2475 , average reward of last 25 episode 21.84\n",
      "episode 2500 , average reward of last 25 episode 23.24\n",
      "episode 2525 , average reward of last 25 episode 22.8\n",
      "episode 2550 , average reward of last 25 episode 21.16\n",
      "episode 2575 , average reward of last 25 episode 20.2\n",
      "episode 2600 , average reward of last 25 episode 22.32\n",
      "episode 2625 , average reward of last 25 episode 23.72\n",
      "episode 2650 , average reward of last 25 episode 21.96\n",
      "episode 2675 , average reward of last 25 episode 23.08\n",
      "episode 2700 , average reward of last 25 episode 23.96\n",
      "episode 2725 , average reward of last 25 episode 24.04\n",
      "episode 2750 , average reward of last 25 episode 21.08\n",
      "episode 2775 , average reward of last 25 episode 20.28\n",
      "episode 2800 , average reward of last 25 episode 21.8\n",
      "episode 2825 , average reward of last 25 episode 20.56\n",
      "episode 2850 , average reward of last 25 episode 22.0\n",
      "episode 2875 , average reward of last 25 episode 21.88\n",
      "episode 2900 , average reward of last 25 episode 21.16\n",
      "episode 2925 , average reward of last 25 episode 22.08\n",
      "episode 2950 , average reward of last 25 episode 21.6\n",
      "episode 2975 , average reward of last 25 episode 23.28\n",
      "episode 3000 , average reward of last 25 episode 22.6\n",
      "Saved Model\n",
      "episode 3025 , average reward of last 25 episode 21.8\n",
      "episode 3050 , average reward of last 25 episode 21.96\n",
      "episode 3075 , average reward of last 25 episode 20.4\n",
      "episode 3100 , average reward of last 25 episode 21.8\n",
      "episode 3125 , average reward of last 25 episode 19.44\n",
      "episode 3150 , average reward of last 25 episode 21.36\n",
      "episode 3175 , average reward of last 25 episode 20.12\n",
      "episode 3200 , average reward of last 25 episode 22.96\n",
      "episode 3225 , average reward of last 25 episode 22.0\n",
      "episode 3250 , average reward of last 25 episode 19.48\n",
      "episode 3275 , average reward of last 25 episode 21.88\n",
      "episode 3300 , average reward of last 25 episode 22.36\n",
      "episode 3325 , average reward of last 25 episode 22.6\n",
      "episode 3350 , average reward of last 25 episode 19.72\n",
      "episode 3375 , average reward of last 25 episode 21.16\n",
      "episode 3400 , average reward of last 25 episode 23.44\n",
      "episode 3425 , average reward of last 25 episode 20.96\n",
      "episode 3450 , average reward of last 25 episode 21.2\n",
      "episode 3475 , average reward of last 25 episode 22.6\n",
      "episode 3500 , average reward of last 25 episode 23.0\n",
      "episode 3525 , average reward of last 25 episode 22.48\n",
      "episode 3550 , average reward of last 25 episode 22.48\n",
      "episode 3575 , average reward of last 25 episode 22.36\n",
      "episode 3600 , average reward of last 25 episode 21.88\n",
      "episode 3625 , average reward of last 25 episode 22.32\n",
      "episode 3650 , average reward of last 25 episode 20.96\n",
      "episode 3675 , average reward of last 25 episode 20.92\n",
      "episode 3700 , average reward of last 25 episode 21.16\n",
      "episode 3725 , average reward of last 25 episode 20.16\n",
      "episode 3750 , average reward of last 25 episode 21.16\n",
      "episode 3775 , average reward of last 25 episode 22.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3800 , average reward of last 25 episode 21.56\n",
      "episode 3825 , average reward of last 25 episode 21.08\n",
      "episode 3850 , average reward of last 25 episode 20.92\n",
      "episode 3875 , average reward of last 25 episode 21.56\n",
      "episode 3900 , average reward of last 25 episode 21.76\n",
      "episode 3925 , average reward of last 25 episode 21.28\n",
      "episode 3950 , average reward of last 25 episode 20.72\n",
      "episode 3975 , average reward of last 25 episode 21.4\n",
      "episode 4000 , average reward of last 25 episode 21.52\n",
      "Saved Model\n",
      "episode 4025 , average reward of last 25 episode 22.32\n",
      "episode 4050 , average reward of last 25 episode 19.36\n",
      "episode 4075 , average reward of last 25 episode 21.6\n",
      "episode 4100 , average reward of last 25 episode 23.0\n",
      "episode 4125 , average reward of last 25 episode 20.28\n",
      "episode 4150 , average reward of last 25 episode 20.4\n",
      "episode 4175 , average reward of last 25 episode 22.96\n",
      "episode 4200 , average reward of last 25 episode 22.68\n",
      "episode 4225 , average reward of last 25 episode 22.08\n",
      "episode 4250 , average reward of last 25 episode 21.4\n",
      "episode 4275 , average reward of last 25 episode 21.4\n",
      "episode 4300 , average reward of last 25 episode 23.32\n",
      "episode 4325 , average reward of last 25 episode 21.6\n",
      "episode 4350 , average reward of last 25 episode 21.88\n",
      "episode 4375 , average reward of last 25 episode 20.88\n",
      "episode 4400 , average reward of last 25 episode 21.68\n",
      "episode 4425 , average reward of last 25 episode 22.96\n",
      "episode 4450 , average reward of last 25 episode 22.4\n",
      "episode 4475 , average reward of last 25 episode 20.8\n",
      "episode 4500 , average reward of last 25 episode 21.48\n",
      "episode 4525 , average reward of last 25 episode 21.44\n",
      "episode 4550 , average reward of last 25 episode 21.0\n",
      "episode 4575 , average reward of last 25 episode 21.64\n",
      "episode 4600 , average reward of last 25 episode 21.64\n",
      "episode 4625 , average reward of last 25 episode 22.28\n",
      "episode 4650 , average reward of last 25 episode 23.56\n",
      "episode 4675 , average reward of last 25 episode 22.56\n",
      "episode 4700 , average reward of last 25 episode 22.08\n",
      "episode 4725 , average reward of last 25 episode 22.28\n",
      "episode 4750 , average reward of last 25 episode 21.08\n",
      "episode 4775 , average reward of last 25 episode 20.68\n",
      "episode 4800 , average reward of last 25 episode 22.84\n",
      "episode 4825 , average reward of last 25 episode 20.32\n",
      "episode 4850 , average reward of last 25 episode 22.32\n",
      "episode 4875 , average reward of last 25 episode 20.6\n",
      "episode 4900 , average reward of last 25 episode 21.44\n",
      "episode 4925 , average reward of last 25 episode 21.56\n",
      "episode 4950 , average reward of last 25 episode 21.92\n",
      "episode 4975 , average reward of last 25 episode 20.08\n",
      "episode 5000 , average reward of last 25 episode 23.44\n",
      "Saved Model\n",
      "episode 5025 , average reward of last 25 episode 22.84\n",
      "episode 5050 , average reward of last 25 episode 22.28\n",
      "episode 5075 , average reward of last 25 episode 20.32\n",
      "episode 5100 , average reward of last 25 episode 21.2\n",
      "episode 5125 , average reward of last 25 episode 22.6\n",
      "episode 5150 , average reward of last 25 episode 22.56\n",
      "episode 5175 , average reward of last 25 episode 21.96\n",
      "episode 5200 , average reward of last 25 episode 22.36\n",
      "episode 5225 , average reward of last 25 episode 20.84\n",
      "episode 5250 , average reward of last 25 episode 20.24\n",
      "episode 5275 , average reward of last 25 episode 20.16\n",
      "episode 5300 , average reward of last 25 episode 22.12\n",
      "episode 5325 , average reward of last 25 episode 22.32\n",
      "episode 5350 , average reward of last 25 episode 22.44\n",
      "episode 5375 , average reward of last 25 episode 21.56\n",
      "episode 5400 , average reward of last 25 episode 21.52\n",
      "episode 5425 , average reward of last 25 episode 22.32\n",
      "episode 5450 , average reward of last 25 episode 20.88\n",
      "episode 5475 , average reward of last 25 episode 21.48\n",
      "episode 5500 , average reward of last 25 episode 21.68\n",
      "episode 5525 , average reward of last 25 episode 21.08\n",
      "episode 5550 , average reward of last 25 episode 22.48\n",
      "episode 5575 , average reward of last 25 episode 22.28\n",
      "episode 5600 , average reward of last 25 episode 21.96\n",
      "episode 5625 , average reward of last 25 episode 20.76\n",
      "episode 5650 , average reward of last 25 episode 22.08\n",
      "episode 5675 , average reward of last 25 episode 21.2\n",
      "episode 5700 , average reward of last 25 episode 22.56\n",
      "episode 5725 , average reward of last 25 episode 23.04\n",
      "episode 5750 , average reward of last 25 episode 22.64\n",
      "episode 5775 , average reward of last 25 episode 21.96\n",
      "episode 5800 , average reward of last 25 episode 21.56\n",
      "episode 5825 , average reward of last 25 episode 21.44\n",
      "episode 5850 , average reward of last 25 episode 21.88\n",
      "episode 5875 , average reward of last 25 episode 22.0\n",
      "episode 5900 , average reward of last 25 episode 20.36\n",
      "episode 5925 , average reward of last 25 episode 21.4\n",
      "episode 5950 , average reward of last 25 episode 21.72\n",
      "episode 5975 , average reward of last 25 episode 22.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 16:41:29.896525 140125394077504 deprecation.py:323] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6000 , average reward of last 25 episode 22.88\n",
      "Saved Model\n",
      "episode 6025 , average reward of last 25 episode 22.72\n",
      "episode 6050 , average reward of last 25 episode 22.28\n",
      "episode 6075 , average reward of last 25 episode 21.92\n",
      "episode 6100 , average reward of last 25 episode 20.44\n",
      "episode 6125 , average reward of last 25 episode 21.84\n",
      "episode 6150 , average reward of last 25 episode 22.0\n",
      "episode 6175 , average reward of last 25 episode 22.28\n",
      "episode 6200 , average reward of last 25 episode 20.6\n",
      "episode 6225 , average reward of last 25 episode 22.92\n",
      "episode 6250 , average reward of last 25 episode 22.4\n",
      "episode 6275 , average reward of last 25 episode 19.88\n",
      "episode 6300 , average reward of last 25 episode 22.28\n",
      "episode 6325 , average reward of last 25 episode 23.08\n",
      "episode 6350 , average reward of last 25 episode 22.4\n",
      "episode 6375 , average reward of last 25 episode 20.96\n",
      "episode 6400 , average reward of last 25 episode 22.64\n",
      "episode 6425 , average reward of last 25 episode 21.52\n",
      "episode 6450 , average reward of last 25 episode 20.96\n",
      "episode 6475 , average reward of last 25 episode 22.8\n",
      "episode 6500 , average reward of last 25 episode 22.2\n",
      "episode 6525 , average reward of last 25 episode 22.16\n",
      "episode 6550 , average reward of last 25 episode 21.88\n",
      "episode 6575 , average reward of last 25 episode 21.56\n",
      "episode 6600 , average reward of last 25 episode 22.56\n",
      "episode 6625 , average reward of last 25 episode 21.24\n",
      "episode 6650 , average reward of last 25 episode 23.96\n",
      "episode 6675 , average reward of last 25 episode 21.36\n",
      "episode 6700 , average reward of last 25 episode 20.56\n",
      "episode 6725 , average reward of last 25 episode 19.84\n",
      "episode 6750 , average reward of last 25 episode 22.92\n",
      "episode 6775 , average reward of last 25 episode 20.64\n",
      "episode 6800 , average reward of last 25 episode 22.28\n",
      "episode 6825 , average reward of last 25 episode 23.0\n",
      "episode 6850 , average reward of last 25 episode 23.16\n",
      "episode 6875 , average reward of last 25 episode 22.72\n",
      "episode 6900 , average reward of last 25 episode 22.24\n",
      "episode 6925 , average reward of last 25 episode 23.44\n",
      "episode 6950 , average reward of last 25 episode 23.28\n",
      "episode 6975 , average reward of last 25 episode 22.28\n",
      "episode 7000 , average reward of last 25 episode 20.08\n",
      "Saved Model\n",
      "episode 7025 , average reward of last 25 episode 20.68\n",
      "episode 7050 , average reward of last 25 episode 22.76\n",
      "episode 7075 , average reward of last 25 episode 22.28\n",
      "episode 7100 , average reward of last 25 episode 21.44\n",
      "episode 7125 , average reward of last 25 episode 20.88\n",
      "episode 7150 , average reward of last 25 episode 22.04\n",
      "episode 7175 , average reward of last 25 episode 21.52\n",
      "episode 7200 , average reward of last 25 episode 22.28\n",
      "episode 7225 , average reward of last 25 episode 20.8\n",
      "episode 7250 , average reward of last 25 episode 21.52\n",
      "episode 7275 , average reward of last 25 episode 21.8\n",
      "episode 7300 , average reward of last 25 episode 21.6\n",
      "episode 7325 , average reward of last 25 episode 22.6\n",
      "episode 7350 , average reward of last 25 episode 21.6\n",
      "episode 7375 , average reward of last 25 episode 22.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-63e08a7c0f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    266\u001b[0m                     \u001b[0;31m#Below we perform the Double-DQN update to the target Q-values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                     \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtargetQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                     \u001b[0mdoubleQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0mtargetQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdoubleQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \"\"\"\n\u001b[1;32m    282\u001b[0m     \u001b[0m_warn_for_nonsequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##GridWorld environment set up\n",
    "\n",
    "#create game objects\n",
    "class gameOb():\n",
    "    def __init__(self,coordinates,size,intensity,channel,reward,name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name\n",
    "        \n",
    "#init game environment       \n",
    "class gameEnv():\n",
    "    def __init__(self,size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        a = self.reset()\n",
    "        plt.imshow(a,interpolation=\"nearest\")\n",
    "        \n",
    "#define game objects\n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
    "        self.objects.append(hero)\n",
    "        goal = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal)\n",
    "        hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole)\n",
    "        goal2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal2)\n",
    "        hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole2)\n",
    "        goal3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal3)\n",
    "        goal4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal4)\n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        return state\n",
    "    \n",
    "#define moving rules    \n",
    "    def moveChar(self,direction):\n",
    "        # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY-2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX-2:\n",
    "            hero.x += 1     \n",
    "        self.objects[0] = hero\n",
    "        \n",
    "#define position    \n",
    "    def newPosition(self):\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x,objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x,objectA.y))\n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)),replace=False)\n",
    "        return points[location]\n",
    "    \n",
    "#checking goal    \n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n",
    "                else: \n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n",
    "                return other.reward,False\n",
    "        return 0.0,False\n",
    "    \n",
    "#display gaming environment    \n",
    "    def renderEnv(self):\n",
    "        #a = np.zeros([self.sizeY,self.sizeX,3])\n",
    "        a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
    "        a[1:-1,1:-1,:] = 0\n",
    "        for item in self.objects:\n",
    "            a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1,item.channel] = item.intensity\n",
    "        b = cv2.resize(a[:,:,0],(84,84),interpolation=cv2.INTER_NEAREST)\n",
    "        c = cv2.resize(a[:,:,1],(84,84),interpolation=cv2.INTER_NEAREST)\n",
    "        d = cv2.resize(a[:,:,2],(84,84),interpolation=cv2.INTER_NEAREST)\n",
    "        a = np.stack([b,c,d],axis=2)\n",
    "        return a\n",
    "    \n",
    "#moving and return values\n",
    "    def step(self,action):\n",
    "        self.moveChar(action)\n",
    "        reward,done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        return state,reward,done\n",
    "\n",
    "#init gridworld environment\n",
    "env = gameEnv(size=5)\n",
    "\n",
    "\n",
    "##Value network\n",
    "\n",
    "#define Q-network\n",
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=512,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it \n",
    "        #into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        #advantage outputs = number of action outputs\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        #value has only one output.\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,reduction_indices=1,keep_dims=True))\n",
    "        #find max Q\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference \n",
    "        #between the target and prediction Q values.\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "        \n",
    "\n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])\n",
    "        \n",
    "        \n",
    "#for easier process\n",
    "def processState(states):\n",
    "    return np.reshape(states,[21168])\n",
    "\n",
    "#update target DQN\n",
    "#tau is the rate to update target network toward primary network\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 1000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 1000#How many episodes of game environment to train network with.\n",
    "pre_train_steps = 1000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "saver = tf.train.Saver()\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "#%%\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(num_episodes+1):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    A = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    doubleQ = Q[range(batch_size),A]\n",
    "                    targetQ = trainBatch[:,2] + y*doubleQ\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "                break\n",
    "        \n",
    "        #Get all experiences from this episode and discount their rewards.\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model.\n",
    "\n",
    "        if i>0 and i % 25 == 0:\n",
    "            print('episode',i,', average reward of last 25 episode',np.mean(rList[-25:]))\n",
    "        if i>0 and i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")            \n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
