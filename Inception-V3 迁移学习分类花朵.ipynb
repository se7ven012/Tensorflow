{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 16:57:43.250150 140503674373952 deprecation.py:323] From <ipython-input-1-ac1fa7eb1403>:56: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: roses. total 300 images.\n",
      "100 images processed.\n",
      "200 images processed.\n",
      "300 images processed.\n",
      "processing: daisy. total 300 images.\n",
      "100 images processed.\n",
      "200 images processed.\n",
      "300 images processed.\n",
      "processing: tulips. total 300 images.\n",
      "100 images processed.\n",
      "200 images processed.\n",
      "300 images processed.\n",
      "processing: dandelion. total 300 images.\n",
      "100 images processed.\n",
      "200 images processed.\n",
      "300 images processed.\n",
      "processing: sunflowers. total 300 images.\n",
      "100 images processed.\n",
      "200 images processed.\n",
      "300 images processed.\n"
     ]
    }
   ],
   "source": [
    "##数据预处理\n",
    "# 这是一个花朵识别的demo，数据集包含了5种花朵，数据都是脏数据 需要预处理\n",
    "# 这里用到的数据集:http://download.tensorflow.org/example_images/flower_photos.tgz\n",
    "# 注意这个数据集处理需要大概16G+的内存 否则会出现MemError，这一问题可通过减少图片数量来解决\n",
    "\n",
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "# 原始输入数据的目录，这个目录下有5个子目录，每个子目录底下保存这属于该类别的所有图片。\n",
    "INPUT_DATA = '/home/se7ven/Desktop/Inception-v3/datasets/flower_photos'\n",
    "# 输出文件地址。我们将整理后的图片数据通过numpy的格式保存。\n",
    "OUTPUT_FILE = '/home/se7ven/Desktop/Inception-v3/datasets/flower_processed_data.npy'\n",
    "\n",
    "# 测试数据和验证数据比例。\n",
    "VALIDATION_PERCENTAGE = 10\n",
    "TEST_PERCENTAGE = 10\n",
    "\n",
    "\n",
    "# 读取数据并将数据分割成训练数据、验证数据和测试数据。\n",
    "def create_image_lists(sess, testing_percentage, validation_percentage):\n",
    "    sub_dirs = [x[0] for x in os.walk(INPUT_DATA)]\n",
    "    is_root_dir = True\n",
    "    \n",
    "    # 初始化各个数据集。\n",
    "    training_images = []\n",
    "    training_labels = []\n",
    "    validation_images = []\n",
    "    validation_labels = []\n",
    "    testing_images = []\n",
    "    testing_labels = []\n",
    "    current_label = 0\n",
    "    \n",
    "    # 读取所有的子目录。\n",
    "    for sub_dir in sub_dirs:\n",
    "        if is_root_dir:        # sub_dirs第一个元素为根目录路径\n",
    "            is_root_dir = False\n",
    "            continue\n",
    "\n",
    "        # 获取一个子目录中所有的图片文件。\n",
    "        extensions = ['jpg']\n",
    "        file_list = []\n",
    "        dir_name = os.path.basename(sub_dir)    # 返回path最后的文件名，/或\\结尾则返回空值\n",
    "        for extension in extensions:\n",
    "            file_glob = os.path.join(INPUT_DATA, dir_name, '*.' + extension)     #  ../../../datasets/flower_photos\\daisy\\*.jpg\n",
    "            file_list.extend(glob.glob(file_glob))    # 匹配所有符合条件文件，并以list返回\n",
    "        if not file_list: continue\n",
    "        print(\"processing: %s. total %d images.\"% (dir_name, len(file_list)))\n",
    "        \n",
    "        i = 0\n",
    "        # 处理图片数据。\n",
    "        for file_name in file_list:\n",
    "            i += 1\n",
    "            # 读取并解析图片，将图片转化为299*299以方便inception-v3模型来处理。\n",
    "            image_raw_data = gfile.FastGFile(file_name, 'rb').read()\n",
    "            #解码\n",
    "            image = tf.image.decode_jpeg(image_raw_data)\n",
    "            #如果图片格式不是float32则转为float32\n",
    "            if image.dtype != tf.int32:\n",
    "                image = tf.image.convert_image_dtype(image, dtype=tf.int32)\n",
    "            #将图片源数据转为299*299\n",
    "            image = tf.image.resize_images(image, [299, 299])\n",
    "            #得到此图片的数据\n",
    "            image_value = sess.run(image)\n",
    "            \n",
    "            #按概率随机分到三个数据集中\n",
    "            chance = np.random.randint(100)\n",
    "            if chance < validation_percentage:\n",
    "                validation_images.append(image_value)\n",
    "                validation_labels.append(current_label)\n",
    "            elif chance < (testing_percentage + validation_percentage):\n",
    "                testing_images.append(image_value)\n",
    "                testing_labels.append(current_label)\n",
    "            else:\n",
    "                training_images.append(image_value)\n",
    "                training_labels.append(current_label)\n",
    "            if i % 100 == 0:\n",
    "                print(i, \"images processed.\")\n",
    "        #处理完此种品种就将标签+1\n",
    "        current_label += 1\n",
    "    \n",
    "    # 将训练数据随机打乱以获得更好的训练效果。\n",
    "    # 这里的get_state记录下数组被打乱的操作，set_state读取信息后可进行同样的操作，保证labels不变\n",
    "    # 用法参见https://blog.csdn.net/mr_muli/article/details/80913915\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(training_images)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(training_labels)\n",
    "    \n",
    "    return np.asarray([training_images, training_labels,\n",
    "                       validation_images, validation_labels,\n",
    "                       testing_images, testing_labels])\n",
    "\n",
    "\n",
    "# 运行数据处理过程\n",
    "with tf.Session() as sess:\n",
    "    processed_data = create_image_lists(sess, TEST_PERCENTAGE, VALIDATION_PERCENTAGE)\n",
    "    # 通过numpy格式保存处理后的数据。\n",
    "    np.save(OUTPUT_FILE, processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206 training examples, 160 validation examples and 134 testing examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 17:15:00.145314 140491283457856 deprecation.py:323] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0829 17:15:01.903965 140491283457856 deprecation.py:506] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0829 17:15:07.319084 140491283457856 deprecation.py:323] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tuned variables from /home/se7ven/Desktop/Inception-v3/datasets/inception_v3.ckpt\n",
      "Step 0: Training loss is 2.0 Validation accuracy = 20.0%\n",
      "Step 30: Training loss is 1.9 Validation accuracy = 21.3%\n",
      "Step 60: Training loss is 1.6 Validation accuracy = 41.9%\n",
      "Step 90: Training loss is 1.0 Validation accuracy = 79.4%\n",
      "Step 120: Training loss is 0.5 Validation accuracy = 85.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 17:16:30.200701 140491283457856 deprecation.py:323] From /home/se7ven/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150: Training loss is 0.3 Validation accuracy = 88.7%\n",
      "Step 180: Training loss is 0.2 Validation accuracy = 90.0%\n",
      "Step 210: Training loss is 0.2 Validation accuracy = 90.0%\n",
      "Step 240: Training loss is 0.2 Validation accuracy = 87.5%\n",
      "Step 270: Training loss is 0.2 Validation accuracy = 90.0%\n",
      "Step 299: Training loss is 0.2 Validation accuracy = 92.5%\n",
      "Final test accuracy = 89.6%\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "import tensorflow.contrib.slim as slim\n",
    "# 加载通过TensorFlow-Slim定义好的inception_v3模型。\n",
    "import tensorflow.contrib.slim.python.slim.nets.inception_v3 as inception_v3\n",
    "\n",
    "# 1. 定义训练过程中将要使用到的常量。\n",
    "# 处理好之后的数据文件。\n",
    "INPUT_DATA = '/home/se7ven/Desktop/Inception-v3/datasets/flower_processed_data.npy'\n",
    "# 保存训练好的模型的路径。这里可以将使用新数据训练得到的模型保存下来，如果计算充足，\n",
    "# 还可以在训练完最后的全连接层之后再训练所有的网络层，使模型更贴近新数据。\n",
    "TRAIN_FILE = 'train_dir/model'\n",
    "# 谷歌提供的训练好的模型文件地址。因为GitHub无法保存大于100M的文件，所以\n",
    "# 在运行时需要先自行从Google下载inception_v3.ckpt文件。\n",
    "CKPT_FILE = '/home/se7ven/Desktop/Inception-v3/datasets/inception_v3.ckpt'\n",
    "\n",
    "# 定义训练中使用的参数。\n",
    "LEARNING_RATE = 0.0001\n",
    "STEPS = 500\n",
    "BATCH = 32\n",
    "N_CLASSES = 5\n",
    "\n",
    "# 不需要从谷歌训练好的模型中加载的参数。这里就是最后的全连接层。这里给出的是参数的前缀\n",
    "CHECKPOINT_EXCLUDE_SCOPES = 'InceptionV3/Logits,InceptionV3/AuxLogits'\n",
    "# 需要训练的网络层参数明层，在fine-tuning的过程中就是最后的全联接层。这里给出的是参数的前缀\n",
    "TRAINABLE_SCOPES = 'InceptionV3/Logits,InceptionV3/AuxLogit'\n",
    "\n",
    "\n",
    "# 2. 获取所有需要从谷歌训练好的模型中加载的参数。\n",
    "def get_tuned_variables():\n",
    "    ##不需要加载的范围\n",
    "    exclusions = [scope.strip() for scope in CHECKPOINT_EXCLUDE_SCOPES.split(',')]\n",
    "    # 初始化需要加载的参数\n",
    "    variables_to_restore = []\n",
    "    \n",
    "    # 遍历模型中所有的参数，然后判断是否需要从加载列表中移除。\n",
    "    for var in slim.get_model_variables():\n",
    "        # 先指定为不需要移除\n",
    "        excluded = False\n",
    "        # 遍历exclusions，如果在exclusions中，就指定为需要移除\n",
    "        for exclusion in exclusions:\n",
    "            if var.op.name.startswith(exclusion):\n",
    "                excluded = True\n",
    "                break\n",
    "        # 如果遍历完后还是不需要移除，就把参数加到列表里        \n",
    "        if not excluded:\n",
    "            variables_to_restore.append(var)\n",
    "    return variables_to_restore\n",
    "\n",
    "\n",
    "# 3. 获取所有需要训练的变量列表。\n",
    "def get_trainable_variables():    \n",
    "    scopes = [scope.strip() for scope in TRAINABLE_SCOPES.split(',')]\n",
    "    variables_to_train = []\n",
    "    \n",
    "    # 枚举所有需要训练的参数前缀，并通过这些前缀找到所有需要训练的参数。\n",
    "    for scope in scopes:\n",
    "        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "        variables_to_train.extend(variables)\n",
    "    return variables_to_train\n",
    "\n",
    "\n",
    "# 4. 定义训练过程\n",
    "def main():\n",
    "    \n",
    "    # 这里是因为numpy的版本更新了 如果不allow_pickle=True会报错，于是先打开，再改回来\n",
    "    np_load_old = np.load\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "    \n",
    "    # 0. 加载预处理好的数据。\n",
    "    processed_data = np.load(INPUT_DATA)\n",
    "    \n",
    "    # restore np.load for future normal usage\n",
    "    np.load = np_load_old\n",
    "    \n",
    "    training_images = processed_data[0]\n",
    "    n_training_example = len(training_images)\n",
    "    training_labels = processed_data[1]\n",
    "    \n",
    "    validation_images = processed_data[2]\n",
    "    validation_labels = processed_data[3]\n",
    "    \n",
    "    testing_images = processed_data[4]\n",
    "    testing_labels = processed_data[5]\n",
    "    print(\"%d training examples, %d validation examples and %d testing examples.\" % (\n",
    "        n_training_example, len(validation_labels), len(testing_labels)))\n",
    "\n",
    "    # 1. 定义inception-v3的输入输出的数据格式\n",
    "    images = tf.placeholder(tf.float32, [None, 299, 299, 3], name='input_images')\n",
    "    labels = tf.placeholder(tf.int64, [None], name='labels')\n",
    "    \n",
    "    # 2. 定义前向传播、损失函数、反向传播\n",
    "    # 定义inception-v3模型。因为谷歌给出的只有模型参数取值，所以这里\n",
    "    # 需要在这个代码中定义inception-v3的模型结构。虽然理论上需要区分训练和\n",
    "    # 测试中使用到的模型，也就是说在测试时应该使用is_training=False，但是\n",
    "    # 因为预先训练好的inception-v3模型中使用的batch normalization参数与\n",
    "    # 新的数据会有出入，所以这里直接使用同一个模型来做测试。\n",
    "    # 可修改num_class = None\n",
    "    with slim.arg_scope(inception_v3.inception_v3_arg_scope()):\n",
    "        logits, _ = inception_v3.inception_v3(\n",
    "            images, num_classes=N_CLASSES, is_training=True)\n",
    "    \n",
    "    # 获取需要训练的变量\n",
    "    trainable_variables = get_trainable_variables()    # 自定义函数，获取需要训练的变量\n",
    "    # 定义损失\n",
    "    tf.losses.softmax_cross_entropy(\n",
    "        tf.one_hot(labels, N_CLASSES), logits, weights=1.0)\n",
    "    total_loss = tf.losses.get_total_loss()\n",
    "    \n",
    "    # 定义训练过程(反向传播)\n",
    "    train_step = tf.train.RMSPropOptimizer(LEARNING_RATE).minimize(total_loss)\n",
    "    \n",
    "    # 计算正确率。\n",
    "    with tf.name_scope('evaluation'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), labels)\n",
    "        evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                \n",
    "    # 定义加载Google训练好的Inception-v3模型的Saver。\n",
    "    # (重新定义load_fn函数，从文件中获取参数，获取指定的变量，忽略缺省值)\n",
    "    load_fn = slim.assign_from_checkpoint_fn(CKPT_FILE,get_tuned_variables(),ignore_missing_vars=True)\n",
    "    \n",
    "    # 3. 建立会话，训练模型\n",
    "    # 定义保存新的训练好的模型的函数\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # 初始化没有加载进来的变量。注意这个过程要在加载模型之前，\n",
    "        # 否则初始化过程会将已经加载好的变量重新赋值。\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        # 加载谷歌已经训练好的模型。\n",
    "        print('Loading tuned variables from %s' % CKPT_FILE)\n",
    "        load_fn(sess)\n",
    "            \n",
    "        start = 0\n",
    "        end = BATCH\n",
    "        for i in range(STEPS):\n",
    "            # 运行训练过程，这里不会更新全部参数，只会更新指定部分的参数\n",
    "            _, loss = sess.run([train_step, total_loss], feed_dict={\n",
    "                images: training_images[start:end], \n",
    "                labels: training_labels[start:end]})\n",
    "\n",
    "            # 间断地保存模型，并在验证集上验证\n",
    "            if i % 80 == 0 or i + 1 == STEPS:\n",
    "                saver.save(sess, TRAIN_FILE, global_step=i)\n",
    "                validation_accuracy = sess.run(evaluation_step, feed_dict={\n",
    "                    images: validation_images, labels: validation_labels})\n",
    "                print('Step %d: Training loss is %.1f Validation accuracy = %.1f%%' % (\n",
    "                    i, loss, validation_accuracy * 100.0))\n",
    "                         \n",
    "            # 更新起始和末尾,获取下一个batch\n",
    "            start = end\n",
    "            if start == n_training_example:\n",
    "                start = 0\n",
    "            end = start + BATCH\n",
    "            if end > n_training_example:\n",
    "                end = n_training_example\n",
    "            \n",
    "        # 训练完了在测试集上测试正确率\n",
    "        test_accuracy = sess.run(evaluation_step, feed_dict={\n",
    "            images: testing_images, labels: testing_labels})\n",
    "        print('Final test accuracy = %.1f%%' % (test_accuracy * 100))\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "#最后的准确度有些低， 因为只取了比较少的数据进行训练"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
